# 🤖 AI with MLKit

CamerAwesome providess a stream of images that you can use to make image analysis.

The ```example``` folder contains two examples using it with MLKit:
- ```ai_analysis_barcode.dart``` reads barcodes
- ```ai_analysis_faces.dart``` detects if there is a face on the camera feed and draws its contours when there is one

Next parts of this doc will go through them.

## Reading barcodes

![Read barcodes](/img/barcode_ai.gif)

First thing you need to do is adding MLKit's barcode scanner to your pubspec.yaml:
``` yaml
google_mlkit_barcode_scanning: ^0.5.0
```

You will need a `BarcodeScanner`. You can get one with:
``` dart
final _barcodeScanner = BarcodeScanner(formats: [BarcodeFormat.all]);
```
You might not want to read all `BarcodeFormat`, so feel free to be more selective.

Now you can set up CamerAwesome.

### Setting up CamerAwesome

This is the builder of the ```ai_analysis_barcode.dart``` example:

``` dart
CameraAwesomeBuilder.custom(
  // 1.
  saveConfig: SaveConfig.photo(pathBuilder: () => _path(CaptureMode.photo)),
  // 2.
  onImageForAnalysis: (AnalysisImage img) => _analysisImagesController.add(img),
  // 3.
  imageAnalysisConfig: AnalysisConfig(
    outputFormat: InputAnalysisImageFormat.nv21,
    width: 1024,
  ),
  // 4.
  builder: (cameraModeState, previewSize, previewRect) {
    // 5.
    return _BarcodeDisplayWidget(
      barcodesStream: _barcodesStream,
      scrollController: _scrollController,
    );
  },
)
```
Let's go through this code snippet:
1. `CameraAwesomeBuilder` needs a `SaveConfig` to eventually save pictures or videos. It provides the path where to save them.

2. ```onImageForAnalysis``` is called whenever a new ImageAnalysis is available. That's where you might want to make your analysis.
Yet, since you might receive a lot of images, you might want to debounce these. That's why we use a stream here: ```_analysisImagesController.add(img)```.

3. Pass the ```AnalysisConfig``` you want for both iOS & Android.

4. Build your interface with the ```builder```.

5. Here, we simply made a widget that displays all the barcodes scanned in a ```ListView```.

The interesting part lies in the images stream listener that we've set in ````initState()```:

``` dart
@override
void initState() {
    // Analyze images every 300 seconds only, to reduce impact on performances
    _analysisSubscription = _analysisImagesController.stream
        .bufferTime(const Duration(milliseconds: 300))
        .map((event) => event.isNotEmpty ? event.last : null)
        .listen((event) {
      if (event != null) {
        _processImageBarcode(event);
      }
    });
    super.initState();
}
```
Using ```bufferTime()``` and ```map()``` from ```rxdart``` allows us to debounce images received and only handle the last one.
This allows us to analyze images every 300 seconds through ```_processImageBarcode()```.

## Extract barcodes from AnalysisImage

Once you get an `AnalysisImage`, you need to convert it to an object that MLKit can handle and make your processing.

``` dart
Future _processImageBarcode(AnalysisImage img) async {
  // 1.
  final Size imageSize = Size(img.width.toDouble(), img.height.toDouble());
  final InputImageRotation imageRotation =
      InputImageRotation.values.byName(img.rotation.name);

  final planeData = img.planes.map(
    (plane) {
      return InputImagePlaneMetadata(
        bytesPerRow: plane.bytesPerRow,
        height: img.height,
        width: img.width,
      );
    },
  ).toList();

  final InputImage inputImage;
  // 2.
  if (Platform.isIOS) {
    final inputImageData = InputImageData(
      size: imageSize,
      imageRotation: imageRotation, // FIXME: seems to be ignored on iOS...
      inputImageFormat: _inputImageFormat(img.format),
      planeData: planeData,
    );

    // 3.
    final WriteBuffer allBytes = WriteBuffer();
    for (final ImagePlane plane in img.planes) {
      allBytes.putUint8List(plane.bytes);
    }
    final bytes = allBytes.done().buffer.asUint8List();

    inputImage =
        InputImage.fromBytes(bytes: bytes, inputImageData: inputImageData);
  } else {
  // 4.
    inputImage = InputImage.fromBytes(
      bytes: img.nv21Image!,
      inputImageData: InputImageData(
        imageRotation: imageRotation,
        inputImageFormat: InputImageFormat.nv21,
        planeData: planeData,
        size: Size(img.width.toDouble(), img.height.toDouble()),
      ),
    );
  }

  try {
  // 5.
    var recognizedBarCodes = await _barcodeScanner.processImage(inputImage);
    for (Barcode barcode in recognizedBarCodes) {
      debugPrint("Barcode: [${barcode.format}]: ${barcode.rawValue}");
      // 6.
      _addBarcode("[${barcode.format.name}]: ${barcode.rawValue}");
    }
  } catch (error) {
    debugPrint("...sending image resulted error $error");
  }
}
```
Let's break down above code:
1. Get image size, rotation and planes.
2. Obtain an `InputImage`. Note that  the way to do it depends on which platform you are since image formats are not the same.
3. **iOS** - Combine the planes to bytes to get your `InputImage`.
4. **Android** - Directly use the `nv21Image` bytes to get an `InputImage`.
5. Now that you have an `InputImage`, give it to MLKit's `BarcodeScanner` to detect the barcodes.
6. Handle the barcodes detected. Here, we add them to a stream that is used in our UI to show them.


Detecting barcodes is quite straightforward with CamerAwesome.
You just need to convert `AnalysisImage` to an `InputImage` as it is done in this example and MLKit will handle the rest. 👌

See the [complete example](https://github.com/Apparence-io/camera_awesome/blob/master/example/lib/ai_analysis_barcode.dart) if you need more details.

## Face detection

![Read barcodes](/img/face_ai.gif)

With MLKit, detecting faces is very similar to scanning barcodes.
Instead of a ```BarcodeScanner```, you need to use a ```FaceDetector```.
However, if you want to draw a mask on top of the face for example, things get more complicated.
Let's see how it can be done with CamerAwesome in ```ai_analysis_faces.dart```.

First, you need to use MLKit's face detection library:
``` dart
google_mlkit_face_detection: ^0.5.5
```

Next you'll need a `FaceDetector`:
``` dart
final options = FaceDetectorOptions(
  enableContours: true,
  enableClassification: true,
  enableLandmarks: true,
);
late final faceDetector = FaceDetector(options: options);
```

MLKit is now ready, let's setup CamerAwesome.

### CamerAwesome setup

``` dart
// 1.
CameraAwesomeBuilder.awesome(
  // 2.
  saveConfig: SaveConfig.photoAndVideo(
    photoPathBuilder: () => _path(CaptureMode.photo),
    videoPathBuilder: () => _path(CaptureMode.video),
    initialCaptureMode: CaptureMode.photo,
  ),
  // 3.
  onMediaTap: (mediaCapture) => OpenFile.open(mediaCapture.filePath),
  // 4.
  previewFit: CameraPreviewFit.contain,
  aspectRatio: CameraAspectRatios.ratio_1_1,
  sensor: Sensors.front,
  // 5.
  onImageForAnalysis: (AnalysisImage img) => _analysisImagesController.add(img),
  // 6.
  imageAnalysisConfig: AnalysisConfig(
    outputFormat: InputAnalysisImageFormat.nv21,
    width: 512,
  ),
  // 7.
  previewDecoratorBuilder: (state, previewSize, previewRect) {
    return _MyPreviewDecoratorWidget(
      cameraState: state,
      faceDetectionStream: _faceDetectionController,
      previewSize: previewSize,
      previewRect: previewRect,
    );
  },
)
```

1. In the barcodes scanner example, we used the `custom()` constructor of `CameraAwesomeBuilder` because we wanted a special UI.
Here, we just wanted to draw face contours above the preview and keep the built-in UI of CamerAwesome, so we used `awesome()` instead.
2. `CameraAwesomeBuilder` always needs a `SaveConfig` to determine where to save pictures and videos.
3. When using `awesome()` constructor, you must tell how to react to taps on the last media's preview. Here, we just open the file.
4. (Optional) Next arguments determine that we want an aspect ratio of 1:1 that is contained in the screen (CameraPreviewFit.contain) and that the starting sensor is the front one.
5. ```onImageForAnalysis``` is called whenever a new ImageAnalysis is available. That's where you might want to try to detect faces.
Yet, since you might receive a lot of images, you might want to debounce these. That's why we use a stream here: ```_analysisImagesController.add(img)```.
6. Set the ```AnalysisConfig``` you want for Android. The smaller the image, the easier the calculations.
7. Draw your UI on top of the camera preview thanks to `previewDecoratorBuilder`. If you want to draw above the preview, `previewSize` and `previewRect` will be useful.
> Note that with `CameraAwesomeBuilder.custom()` you don't have a `previewDecoratorBuilder`. Use the `builder` property instead.


The images received for image analysis are debounced in ```initState()```:

``` dart
@override
void initState() {
    // Analyze images every 300 seconds only, to reduce impact on performances
    _analysisSubscription = _analysisImagesController.stream
        .bufferTime(const Duration(milliseconds: 300))
        .map((event) => event.isNotEmpty ? event.last : null)
        .listen((event) {
      if (event != null) {
         _analyzeImage(event);
      }
    });
    super.initState();
}
```
Using ```bufferTime()``` and ```map()``` from ```rxdart``` allows us to debounce images received and only handle the last one.
This allows us to analyze images every 300 seconds through ```_analyzeImage()```.

Now let's see how the face detection is done.

### Detecting faces in an AnalysisImage

Most complicated part of the detection is to convert an `AnalysisImage` to an `InputImage`.
Let's see how it's done in the face detection example:

``` dart
Future _analyzeImage(AnalysisImage img) async {
  // 1.
  final Size imageSize = Size(img.width.toDouble(), img.height.toDouble());

  final InputImageRotation imageRotation =
      InputImageRotation.values.byName(img.rotation.name);

  final planeData = img.planes.map(
    (ImagePlane plane) {
      return InputImagePlaneMetadata(
        bytesPerRow: plane.bytesPerRow,
        height: plane.height,
        width: plane.width,
      );
    },
  ).toList();

  final InputImage inputImage;
  // 2.
  if (Platform.isIOS) {
    final inputImageData = InputImageData(
      size: imageSize,
      imageRotation: imageRotation, // FIXME: seems to be ignored on iOS...
      inputImageFormat: inputImageFormat(img.format),
      planeData: planeData,
    );

    // 3.
    final WriteBuffer allBytes = WriteBuffer();
    for (final ImagePlane plane in img.planes) {
      allBytes.putUint8List(plane.bytes);
    }
    final bytes = allBytes.done().buffer.asUint8List();

    inputImage =
        InputImage.fromBytes(bytes: bytes, inputImageData: inputImageData);
  } else {
    // 4.
    inputImage = InputImage.fromBytes(
      bytes: img.nv21Image!,
      inputImageData: InputImageData(
        imageRotation: imageRotation,
        inputImageFormat: InputImageFormat.nv21,
        planeData: planeData,
        size: Size(img.width.toDouble(), img.height.toDouble()),
      ),
    );
  }

  try {
    // 5.
    _faceDetectionController.add(
      FaceDetectionModel(
        faces: await faceDetector.processImage(inputImage),
        absoluteImageSize: inputImage.inputImageData!.size,
        rotation: 0,
        imageRotation: imageRotation,
        cropRect: img.cropRect,
      ),
    );
  } catch (error) {
    debugPrint("...sending image resulted error $error");
  }
}
```
Let's break down above method:
1. Get image size, rotation and planes.
2. Obtain an `InputImage`. Note that the way to do it depends on which platform you are since image formats are not the same.
3. **iOS** - Combine the planes to bytes to get your `InputImage`.
4. **Android** - Directly use the `nv21Image` bytes to get an `InputImage`.
5. Now that you have an `InputImage`, give it to MLKit's `FaceDetector` to eventually detect faces. Next, wrap these in a model that is added to a stream.

The result of the face detection will be handled by a StreamBuilder: each time a new result is emitted, the UI will be updated with this result.


### Drawing face contours

Now that our model is ready, we can look into the UI code.

First, take a look at `_MyPreviewDecoratorWidget`:
``` dart
class _MyPreviewDecoratorWidget extends StatelessWidget {
  final CameraState cameraState;
  final Stream<FaceDetectionModel> faceDetectionStream;
  final PreviewSize previewSize;
  final Rect previewRect;

  const _MyPreviewDecoratorWidget({
    required this.cameraState,
    required this.faceDetectionStream,
    required this.previewSize,
    required this.previewRect,
  });

  @override
  Widget build(BuildContext context) {
    return IgnorePointer(
      child: StreamBuilder(
        // 1.
        stream: cameraState.sensorConfig$,
        builder: (_, snapshot) {
          if (!snapshot.hasData) {
            return const SizedBox();
          } else {
            return StreamBuilder<FaceDetectionModel>(
              // 2.
              stream: faceDetectionStream,
              builder: (_, faceModelSnapshot) {
                if (!faceModelSnapshot.hasData) return const SizedBox();
                // 3.
                return CustomPaint(
                  painter: FaceDetectorPainter(
                    model: faceModelSnapshot.requireData,
                    previewSize: previewSize,
                    previewRect: previewRect,
                    isBackCamera: snapshot.requireData.sensor == Sensors.back,
                  ),
                );
              },
            );
          }
        },
      ),
    );
  }
}
```
Here is a break through of the above code:
1. Listen to the `sensorConfig$` stream. This will be used to determine if it's the front or back camera.
Indeed, image analysis on Android is not mirrored but the preview is. In this case, we will need to make adjustments.
2. Listen to `faceDetectionStream`. Results of the face detection are emitted to this stream and we want to rebuild the UI each time there is a new result.
3. Widgets are not sufficient enough to be able to draw contours out of the box. A CustomPainter is the way to go here.

Now let's see how our `FaceDetectorPainter` works by looking at its `paint()` method.

``` dart
@override
void paint(Canvas canvas, Size size) {
  final croppedSize = model.cropRect == null
      ? model.absoluteImageSize
      : Size(
          // Width and height are inverted
          model.cropRect!.size.height,
          model.cropRect!.size.width,
        );
  final ratioAnalysisToPreview = previewSize.width / croppedSize.width;

  // 1.
  bool flipXY = false;
  if (Platform.isAndroid) {
    // Symmetry for Android since native image analysis is not mirrored but preview is
    // It also handles device rotation
    switch (model.imageRotation) {
      case InputImageRotation.rotation0deg:
        if (isBackCamera) {
          flipXY = true;
          canvas.scale(-1, 1);
          canvas.translate(-size.width, 0);
        } else {
          flipXY = true;
          canvas.scale(-1, -1);
          canvas.translate(-size.width, -size.height);
        }
        break;
      case InputImageRotation.rotation90deg:
        if (isBackCamera) {
          // No changes
        } else {
          canvas.scale(1, -1);
          canvas.translate(0, -size.height);
        }
        break;
      case InputImageRotation.rotation180deg:
        if (isBackCamera) {
          flipXY = true;
          canvas.scale(1, -1);
          canvas.translate(0, -size.height);
        } else {
          flipXY = true;
        }
        break;
      default:
        // 270 or null
        if (isBackCamera) {
          canvas.scale(-1, -1);
          canvas.translate(-size.width, -size.height);
        } else {
          canvas.scale(-1, 1);
          canvas.translate(-size.width, 0);
        }
    }
  }

  // 2.
  for (final Face face in model.faces) {
    // 3.
    Map<FaceContourType, Path> paths = {
      for (var fct in FaceContourType.values) fct: Path()
    };
    // 4.
    face.contours.forEach((contourType, faceContour) {
      if (faceContour != null) {
        // 5.
        paths[contourType]!.addPolygon(
            faceContour.points
                .map(
                  (element) => _croppedPosition(
                    element,
                    croppedSize: croppedSize,
                    painterSize: size,
                    ratio: ratioAnalysisToPreview,
                    flipXY: flipXY,
                  ),
                )
                .toList(),
            true);
        // 6.
        for (var element in faceContour.points) {
          canvas.drawCircle(
            _croppedPosition(
              element,
              croppedSize: croppedSize,
              painterSize: size,
              ratio: ratioAnalysisToPreview,
              flipXY: flipXY,
            ),
            4,
            Paint()..color = Colors.blue,
          );
        }
      }
    });
    // 7.
    paths.removeWhere((key, value) => value.getBounds().isEmpty);

    // 8.
    for (var p in paths.entries) {
      canvas.drawPath(
          p.value,
          Paint()
            ..color = Colors.orange
            ..strokeWidth = 2
            ..style = PaintingStyle.stroke);
    }
  }
}
```
Here is a short break through of the above code:
1. On Android, imageRotation and the fact that the image analysis image is not mirrored on the front camera implies to make some calculations on the canvas to position properly the elements.
2. Handle each face detected
3. Initialize a map of each contourType to an empty Path that will be eventually filled later.
4. Iterate over the contours of the face.
5. Add points of the contour to a Path as a polygon. In a real world scenario, you might want to proceed differently.
6. Draw a circle at each contour's points coordinates in blue.
7. Filter the contours not found.
8. Draw the contours that we found in orange.

You may have noticed the use of `_croppedPosition()` to position the elements on the canvas.
Here is its definition:
``` dart
Offset _croppedPosition(
  Point<int> element, {
  required Size croppedSize,
  required Size painterSize,
  required double ratio,
  required bool flipXY,
}) {
  // 1.
  num imageDiffX;
  num imageDiffY;
  if (Platform.isIOS) {
    imageDiffX = model.absoluteImageSize.width - croppedSize.width;
    imageDiffY = model.absoluteImageSize.height - croppedSize.height;
  } else {
    // 2.
    imageDiffX = model.absoluteImageSize.height - croppedSize.width;
    imageDiffY = model.absoluteImageSize.width - croppedSize.height;
  }

  return (Offset(
            // 3.
            (flipXY ? element.y : element.x).toDouble() - (imageDiffX / 2),
            (flipXY ? element.x : element.y).toDouble() - (imageDiffY / 2),
          ) *
          ratio) // 4.
      .translate(
    // 5.
    (painterSize.width - (croppedSize.width * ratio)) / 2,
    (painterSize.height - (croppedSize.height * ratio)) / 2,
  );
}
```
Let's explain what's going on:
1. The image from image analysis might not reflect what is seen on the camera preview due to a difference between their aspect ratio.
For instance, image analysis could return a picture in 600x800 (ratio 3:4) while the camera preview could be at 1000x1000.
The croppedSize is the part of the imageAnalysis that is visible in the cameraPreview. It would be 600x600 for the above example.
`imageDiffX` and `imageDiffY` are here to transpose MLKit's points coordinates in the part of the image analysis that is visible in the camera preview.
2. absoluteImageSize width and height are inverted on Android.
3. Transposition of the point coordinates in the image analysis to its cropped equivalent.
4. Ratio between the preview and the cropped image size. Example: a point at (300, 300) of the image analysis which size is 600x600 would become (500, 500) for a preview size of 1000x1000.
5. The painter size might be taller or wider than the preview size (or its equivalent, croppedSize * ratio). In these cases, the preview is centered. The position should be translated accordingly.


Drawing on top of the preview is definitively the more complicated part here, but as you saw it is still doable ✌️

See the [complete example](https://github.com/Apparence-io/camera_awesome/blob/master/example/lib/ai_analysis_faces.dart) if you need more details.
